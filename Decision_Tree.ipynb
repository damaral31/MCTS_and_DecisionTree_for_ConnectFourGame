{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d712b728",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "## Introdução às Decision Trees\n",
    "\n",
    "As **Decision Trees** são algoritmos de aprendizagem supervisionada utilizados em tarefas de **classificação** e **regressão**. Funcionam através de uma divisão recursiva dos dados com base em regras de decisão simples, formando uma estrutura em forma de árvore.\n",
    "\n",
    "Cada divisão é guiada por uma métrica de qualidade, sendo a mais comum o **Information Gain**, que mede a redução de **entropia** após uma divisão, ou seja, o quanto uma determinada feature contribui para separar eficazmente as classes.\n",
    "\n",
    "## Aplicação no Projeto\n",
    "\n",
    "Neste projeto, utilizamos uma árvore de decisão para **prever a jogada ótima** (i.e., a coluna a jogar) a partir de um estado do tabuleiro no jogo *4 em linha*. A jogada considerada ótima é aquela que seria escolhida por um agente **Monte Carlo Tree Search (MCTS)** com **10.000 iterações**.\n",
    "\n",
    "## Geração do Dataset\n",
    "\n",
    "Para treinar a árvore de decisão, foi gerado um **dataset supervisionado** com aproximadamente **500 jogos simulados**, a partir dos quais foram extraídas diversas posições intermédias com respetiva jogada ideal anotada.\n",
    "\n",
    "## Evolução da Representação do Dataset\n",
    "\n",
    "### Estrutura Inicial\n",
    "\n",
    "Na fase inicial do projeto, o dataset foi construído com base numa **representação direta do estado do tabuleiro** do jogo *4 em linha*. Cada linha do dataset correspondia a um momento específico de um jogo simulado, e era composta pelos seguintes elementos:\n",
    "\n",
    "- **Um único vetor** que representava o tabuleiro completo:\n",
    "  - Cada célula da grelha era codificada como:\n",
    "    - `0` → célula vazia\n",
    "    - `1` → peça do jogador 1\n",
    "    - `-1` → peça do jogador 2\n",
    "  - A ordem dos elementos era **coluna a coluna**, **do topo para a base**, espelhando a forma como as peças são empilhadas no jogo.\n",
    "- Uma coluna adicional com o **número total de peças** jogadas até ao momento (proxy da profundidade no jogo).\n",
    "- Uma coluna que indicava o **jogador atual** a jogar (1 ou 2).\n",
    "- A coluna de **output** representava a **coluna ideal** para jogar, determinada por um agente **Monte Carlo Tree Search (MCTS)** com 10.000 iterações.\n",
    "\n",
    "Esta estrutura simples era funcional e suficiente para uma primeira abordagem, mas apresentava algumas **limitações**, nomeadamente:\n",
    "\n",
    "- Mistura da informação dos dois jogadores num único vetor, dificultando a distinção clara entre as peças.\n",
    "- Potencial ambiguidade para modelos de aprendizagem que tratam os dados de forma vetorial.\n",
    "- Menor capacidade de generalização para diferentes representações.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### Nova Estrutura do Dataset\n",
    "\n",
    "Inspirado por abordagens usadas no treino de agentes para o jogo **Go**, a representação do estado do jogo foi reformulada para melhor refletir a informação posicional de forma explícita e neutra. O estado do tabuleiro é agora representado da seguinte forma:\n",
    "\n",
    "- **Dois tabuleiros binários**, cada um com as mesmas dimensões do tabuleiro original (por exemplo, 6x7):\n",
    "  - Um tabuleiro representa as posições ocupadas pelo **Jogador 1** (`1` para peça presente, `0` caso contrário).\n",
    "  - O outro representa as posições do **Jogador 2**, com a mesma codificação.\n",
    "- Cada entrada do dataset inclui:\n",
    "  - Os dois vetores resultantes do **flattening** dos tabuleiros.\n",
    "  - Uma feature adicional com o **número total de peças** jogadas até ao momento.\n",
    "  - Uma feature que identifica o **jogador atual** (1 ou -1).\n",
    "  - A **coluna de output** com a jogada ideal sugerida pelo MCTS (com 10.000 simulações).\n",
    "\n",
    "Esta nova representação não só oferece uma **separação clara da informação entre os dois agentes**, como também permite que o modelo **aprenda padrões estratégicos específicos de cada jogador**. A estrutura binária facilita ainda a integração com modelos baseados em regras e em redes neuronais, caso seja pretendida uma extensão futura do projeto.\n",
    "\n",
    "---\n",
    "\n",
    "Na próxima secção será discutida a implementação técnica do algoritmo ID3 utilizado para treinar a árvore de decisão com base neste dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d8df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cel1</th>\n",
       "      <th>cel2</th>\n",
       "      <th>cel3</th>\n",
       "      <th>cel4</th>\n",
       "      <th>cel5</th>\n",
       "      <th>cel6</th>\n",
       "      <th>cel7</th>\n",
       "      <th>cel8</th>\n",
       "      <th>cel9</th>\n",
       "      <th>cel10</th>\n",
       "      <th>...</th>\n",
       "      <th>cel36</th>\n",
       "      <th>cel37</th>\n",
       "      <th>cel38</th>\n",
       "      <th>cel39</th>\n",
       "      <th>cel40</th>\n",
       "      <th>cel41</th>\n",
       "      <th>cel42</th>\n",
       "      <th>pieces</th>\n",
       "      <th>turn</th>\n",
       "      <th>played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cel1  cel2  cel3  cel4  cel5  cel6  cel7  cel8  cel9  cel10  ...  cel36  \\\n",
       "0     0     0     0     0     0     0     0     0     0      0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0      0  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     0      0  ...      0   \n",
       "3     0     0     0     0     0    -1     0     0     0      0  ...     -1   \n",
       "4     0     0     0     0     0     0     0     0     0      0  ...      0   \n",
       "\n",
       "   cel37  cel38  cel39  cel40  cel41  cel42  pieces  turn  played  \n",
       "0     -1     -1      1      1     -1      1      12     1     5.0  \n",
       "1     -1      1      1     -1     -1     -1      13     1     5.0  \n",
       "2     -1     -1      1     -1      1      1       7     1     5.0  \n",
       "3      1      0      1     -1      1     -1      17     1     3.0  \n",
       "4      0      0      0      0     -1      1       5     1     6.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/monte_carlo_data.csv',delimiter=';')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec54c5",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "# Tentativa de Randomização das Posições\n",
    "\n",
    "Durante a fase de geração do dataset, foi inicialmente considerada a ideia de **randomizar posições do tabuleiro** em vez de as obter exclusivamente a partir de jogos completos simulados com agentes MCTS.\n",
    "\n",
    "A motivação por trás desta abordagem era: \n",
    "- Aumentar a diversidade de posições no dataset\n",
    "- Reduzir o tempo necessário para simular jogos completos\n",
    "- Explorar casos de jogo menos prováveis, mas ainda legais\n",
    "\n",
    "### Estratégia Testada\n",
    "\n",
    "O processo consistia em:\n",
    "1. Gerar posições aleatórias válidas (respeitando as regras do 4 em linha, como gravidade das peças)\n",
    "2. Avaliar essas posições usando o agente MCTS com 10.000 iterações\n",
    "3. Adicionar a posição e a jogada recomendada ao dataset\n",
    "\n",
    "Apesar de parecer viável, esta abordagem foi **desaconselhada pelo professor da cadeira** a qual foi, então, descartada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393abb5c",
   "metadata": {},
   "source": [
    "# Algoritmo ID3 - Implementação da Árvore de Decisão\n",
    "\n",
    "Neste capítulo será abordada a implementação do algoritmo **ID3** para construção de Decision Trees , conforme desenvolvido no ficheiro [ID3Tree.py](DecisionTree/ID3Tree.py).\n",
    "\n",
    "A implementação encontra-se encapsulada numa classe denominada `ID3Tree`, a qual integra os métodos essenciais para:\n",
    "- cálculo da entropia de um conjunto de rótulos,\n",
    "- determinação do ganho de informação,\n",
    "- construção recursiva da árvore de decisão,\n",
    "- classificação de novos exemplos.\n",
    "\n",
    "\n",
    "\n",
    "## Método `entropy`\n",
    "\n",
    "O método `entropy` é definido na classe `ID3Tree` e tem como função calcular a **entropia** de um conjunto de rótulos, o que corresponde a uma medida quantitativa da **impureza** ou incerteza inerente a esse conjunto.\n",
    "\n",
    "```python\n",
    "def entropy(self, labels):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a set of labels.\n",
    "    - labels: List of class labels.\n",
    "    \"\"\"\n",
    "    total = len(labels)\n",
    "    counter = Counter(labels)  # Count occurrences of each label\n",
    "    return -sum((count / total) * math.log2(count / total) for count in counter.values())\n",
    "```\n",
    "\n",
    "### Definição e Justificação\n",
    "\n",
    "A entropia é uma métrica fundamental na teoria da informação, utilizada para quantificar a quantidade de incerteza num conjunto de dados. No contexto do algoritmo ID3, é empregue para medir a heterogeneidade dos rótulos em cada subconjunto de dados, servindo de base para a escolha dos atributos que melhor segmentam a informação.\n",
    "\n",
    "Formalmente, a entropia $H(S)$ de um conjunto $S$ contendo $C$ classes distintas é dada por:\n",
    "\n",
    "$$\n",
    "H(S) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "onde $p_i$ representa a proporção de elementos pertencentes à classe $i$ no conjunto $S$.\n",
    "\n",
    "### Análise do Método\n",
    "\n",
    "- **Cálculo do total de amostras**:  \n",
    "  O número total de rótulos no conjunto é determinado através de `total = len(labels)`.\n",
    "\n",
    "- **Contagem da frequência de cada classe**:  \n",
    "  Utiliza-se a estrutura `Counter` da biblioteca `collections` para obter a frequência absoluta de cada classe.\n",
    "\n",
    "- **Cálculo da entropia**:  \n",
    "\n",
    "  Para cada classe, calcula-se a frequência relativa  $ p_i = \\frac{\\text{count}}{\\text{total}} $ e avalia-se o termo $ -p_i \\log_2(p_i) $. A soma destes termos para todas as classes resulta no valor da entropia do conjunto.\n",
    "\n",
    "### Relevância para o Algoritmo ID3\n",
    "\n",
    "O cálculo da entropia é indispensável para o cálculo subsequente do **ganho de informação** (information gain), que avalia a eficácia da segmentação do conjunto de dados por cada atributo. O atributo que proporciona a maior redução da entropia é escolhido para a divisão do nó, conduzindo a uma árvore de decisão mais eficaz e informativa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea712b3d",
   "metadata": {},
   "source": [
    "## Método `id3_train`\n",
    "\n",
    "O método `id3_train` é o núcleo da construção da árvore de decisão, implementando o algoritmo **ID3** de forma recursiva. Este método recebe como entrada o conjunto de dados de treino e a lista de atributos disponíveis para a divisão, e devolve a estrutura da árvore construída.\n",
    "\n",
    "```python\n",
    "def id3_train(self, data, attributes):\n",
    "    \"\"\"\n",
    "    Recursively build the decision tree using the ID3 algorithm.\n",
    "    - data: Training data.\n",
    "    - attributes: List of attributes to consider.\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return self.default  # Return default if no data is available\n",
    "    if len(set(row[-1] for row in data)) == 1:\n",
    "        return data[0][-1]  # Return the label if all data has the same label\n",
    "\n",
    "    # Calculate fitness scores for all attributes\n",
    "    scores = [(self.fitness_for(attr)(data, attr), attr) for attr in attributes]\n",
    "    best_gain, best_attr = max(scores, key=lambda x: x[0][0] if isinstance(x[0], tuple) else x[0])\n",
    "\n",
    "    if self.type_map[best_attr] == 'continuous':\n",
    "        # Handle continuous attributes\n",
    "        threshold = best_gain[1]\n",
    "        node = Node(best_attr, threshold, best_gain[0])  # Create a node with a threshold\n",
    "        above = [row for row in data if row[self.attributes.index(best_attr)] >= threshold]\n",
    "        below = [row for row in data if row[self.attributes.index(best_attr)] < threshold]\n",
    "        return {node: {\n",
    "            '>=': self.id3_train(above, attributes),\n",
    "            '<': self.id3_train(below, attributes)\n",
    "        }}\n",
    "    else:\n",
    "        # Handle discrete attributes\n",
    "        index = self.attributes.index(best_attr)\n",
    "        values = set(row[index] for row in data)\n",
    "        node = Node(best_attr, None, best_gain[0])  # Create a node without a threshold\n",
    "        return {node: {\n",
    "            val: self.id3_train([row for row in data if row[index] == val], [a for a in attributes if a != best_attr])\n",
    "            for val in values\n",
    "        }}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Explicação detalhada\n",
    "\n",
    "1. **Condição de paragem: ausência de dados**\n",
    "\n",
    "   ```python\n",
    "   if not data:\n",
    "       return self.default\n",
    "   ```\n",
    "\n",
    "   Caso o conjunto de dados recebido esteja vazio, a função retorna a classe padrão `self.default`, que normalmente corresponde à classe mais frequente do conjunto de treino inicial. Esta condição impede que o algoritmo tente construir uma árvore com dados inexistentes.\n",
    "\n",
    "2. **Condição de paragem: pureza do nó**\n",
    "\n",
    "   ```python\n",
    "   if len(set(row[-1] for row in data)) == 1:\n",
    "       return data[0][-1]\n",
    "   ```\n",
    "\n",
    "   Se todos os exemplos no conjunto de dados têm o mesmo rótulo (ou seja, o nó é puro), o método retorna esse rótulo. Neste ponto, a construção da árvore para naquele ramo, pois a decisão já está clara.\n",
    "\n",
    "3. **Cálculo do ganho de informação para cada atributo**\n",
    "\n",
    "   ```python\n",
    "   scores = [(self.fitness_for(attr)(data, attr), attr) for attr in attributes]\n",
    "   best_gain, best_attr = max(scores, key=lambda x: x[0][0] if isinstance(x[0], tuple) else x[0])\n",
    "   ```\n",
    "\n",
    "   Para cada atributo na lista de atributos disponíveis, calcula-se a sua \"fitness\" que corresponde ao ganho de informação (information gain) associado à divisão pelo atributo.  \n",
    "   \n",
    "   O método `fitness_for(attr)` retorna a função apropriada para calcular a qualidade da divisão segundo o tipo do atributo (contínuo ou discreto).  \n",
    "   \n",
    "   De seguida, seleciona-se o atributo que maximiza o ganho de informação, pois este atributo permitirá a melhor segmentação dos dados naquele nó.\n",
    "\n",
    "4. **Divisão do conjunto de dados consoante o tipo do atributo**\n",
    "\n",
    "   - **Atributos contínuos**\n",
    "\n",
    "     Quando o atributo é contínuo, é determinado um limiar (`threshold`) que melhor divide o conjunto.\n",
    "\n",
    "     ```python\n",
    "     threshold = best_gain[1]\n",
    "     node = Node(best_attr, threshold, best_gain[0])\n",
    "     ```\n",
    "\n",
    "     Criamos um nó contendo o atributo, o limiar e o valor do ganho.  \n",
    "     \n",
    "     Depois, os dados são particionados em dois subconjuntos:\n",
    "     - `above`: exemplos cujo valor do atributo é maior ou igual ao limiar,\n",
    "     - `below`: exemplos cujo valor do atributo é inferior ao limiar.\n",
    "\n",
    "     Para cada subconjunto, chama-se recursivamente `id3_train` para continuar a construção da árvore:\n",
    "\n",
    "     ```python\n",
    "     return {node: {\n",
    "         '>=': self.id3_train(above, attributes),\n",
    "         '<': self.id3_train(below, attributes)\n",
    "     }}\n",
    "     ```\n",
    "\n",
    "   - **Atributos discretos**\n",
    "\n",
    "     Quando o atributo é discreto, o conjunto de dados é dividido em tantos subconjuntos quanto os valores distintos do atributo:\n",
    "\n",
    "     ```python\n",
    "     index = self.attributes.index(best_attr)\n",
    "     values = set(row[index] for row in data)\n",
    "     node = Node(best_attr, None, best_gain[0])\n",
    "     ```\n",
    "\n",
    "     Para cada valor distinto do atributo, filtra-se o conjunto de dados correspondente e chama-se recursivamente `id3_train` excluindo o atributo já utilizado para evitar ciclos.\n",
    "\n",
    "     ```python\n",
    "     return {node: {\n",
    "         val: self.id3_train([row for row in data if row[index] == val], [a for a in attributes if a != best_attr])\n",
    "         for val in values\n",
    "     }}\n",
    "     ```\n",
    "\n",
    "\n",
    "\n",
    "## Tratamento de dados contínuos vs discretos\n",
    "\n",
    "Embora o dataset principal do projeto, correspondente aos jogos de 4 em linha, seja composto quase exclusivamente por atributos discretos (com valores 0, 1, ou -1 representando os estados das células), à exceção de `pieces`, que descreve número de peças no tabuleiro, foi solicitado no âmbito do trabalho que a árvore de decisão fosse testada também no conjunto de dados clássico **Iris**, que possui atributos contínuos.\n",
    "\n",
    "Por esta razão, tornou-se imperativo que a função `id3_train` distinguisse explicitamente entre atributos contínuos e discretos, aplicando abordagens específicas para cada caso:\n",
    "\n",
    "- Para atributos contínuos, é necessário determinar o melhor limiar (`threshold`) para a divisão do conjunto, partindo os dados em subconjuntos baseados nesse ponto de corte.\n",
    "- Para atributos discretos, o conjunto é particionado em subconjuntos de acordo com os valores categóricos presentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4859ef",
   "metadata": {},
   "source": [
    "\n",
    "### Função `id3_continuous`\n",
    "\n",
    "Esta função avalia o ganho de informação associado a um **atributo contínuo**.\n",
    "\n",
    "```python\n",
    "def id3_continuous(self, data, attribute):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a continuous attribute.\n",
    "    - data: Training data.\n",
    "    - attribute: The attribute to evaluate.\n",
    "    \"\"\"\n",
    "    idx = self.attributes.index(attribute)\n",
    "    values = sorted(set(row[idx] for row in data))  # Unique sorted values of the attribute\n",
    "    if len(values) == 1:\n",
    "        return -1, None  # No split possible if only one unique value\n",
    "\n",
    "    # Calculate potential thresholds\n",
    "    thresholds = [(values[i] + values[i + 1]) / 2 for i in range(len(values) - 1)]\n",
    "    base_entropy = self.entropy([row[-1] for row in data])  # Entropy of the entire dataset\n",
    "\n",
    "    best_gain, best_thresh = -1, None\n",
    "    for t in thresholds:\n",
    "        # Split data into above and below threshold\n",
    "        above = [row for row in data if row[idx] >= t]\n",
    "        below = [row for row in data if row[idx] < t]\n",
    "        p, n = len(above) / len(data), len(below) / len(data)\n",
    "        # Calculate information gain\n",
    "        gain = base_entropy - p * self.entropy([r[-1] for r in above]) - n * self.entropy([r[-1] for r in below])\n",
    "        if gain > best_gain:\n",
    "            best_gain, best_thresh = gain, t\n",
    "    return best_gain, best_thresh\n",
    "```\n",
    "\n",
    "#### Explicação técnica\n",
    "\n",
    "1. **Extração e ordenação dos valores do atributo**\n",
    "\n",
    "   ```python\n",
    "   values = sorted(set(row[idx] for row in data))\n",
    "   ```\n",
    "\n",
    "   São considerados apenas os valores únicos e ordenados do atributo contínuo.\n",
    "\n",
    "2. **Geração de limiares candidatos**\n",
    "\n",
    "   São gerados todos os possíveis pontos médios entre pares consecutivos de valores:\n",
    "\n",
    "   $$\n",
    "   \\text{threshold}_i = \\frac{v_i + v_{i+1}}{2}\n",
    "   $$\n",
    "\n",
    "   Estes limiares são os pontos de corte candidatos para dividir o conjunto de dados.\n",
    "\n",
    "3. **Cálculo do ganho de informação**\n",
    "\n",
    "   Para cada limiar $ t $, divide-se o conjunto em duas partes:\n",
    "   - Acima do limiar: $ D_{\\geq t} $\n",
    "   - Abaixo do limiar: $ D_{< t} $\n",
    "\n",
    "   O ganho de informação é então calculado como:\n",
    "\n",
    "   $$\n",
    "   \\text{Gain}(t) = H(D) - p \\cdot H(D_{\\geq t}) - (1 - p) \\cdot H(D_{< t})\n",
    "   $$\n",
    "\n",
    "   onde $ H(D) $ é a entropia do conjunto total e $ p $ é a proporção de dados em $ D_{\\geq t} $.\n",
    "\n",
    "4. **Resultado**\n",
    "\n",
    "   A função devolve o limiar $ t $ que gera o maior ganho de informação, juntamente com o valor desse ganho.\n",
    "\n",
    "\n",
    "\n",
    "### Função `id3_discrete`\n",
    "\n",
    "Esta função calcula o ganho de informação para atributos **discretos**, ou seja, com um número finito de categorias.\n",
    "\n",
    "```python\n",
    "def id3_discrete(self, data, attribute):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a discrete attribute.\n",
    "    - data: Training data.\n",
    "    - attribute: The attribute to evaluate.\n",
    "    \"\"\"\n",
    "    idx = self.attributes.index(attribute)\n",
    "    base_entropy = self.entropy([row[-1] for row in data])  # Entropy of the entire dataset\n",
    "    values = set(row[idx] for row in data)  # Unique values of the attribute\n",
    "\n",
    "    remainder = 0\n",
    "    for val in values:\n",
    "        # Subset of data where the attribute equals the current value\n",
    "        subset = [row for row in data if row[idx] == val]\n",
    "        remainder += (len(subset) / len(data)) * self.entropy([row[-1] for row in subset])\n",
    "\n",
    "    return base_entropy - remainder, None\n",
    "```\n",
    "\n",
    "#### Explicação técnica\n",
    "\n",
    "Neste caso, o conjunto de dados é particionado em subconjuntos distintos consoante os valores únicos do atributo.\n",
    "\n",
    "O ganho de informação é calculado da seguinte forma:\n",
    "\n",
    "- Entropia inicial: $ H(D) $\n",
    "- Resto (_remainder_):\n",
    "\n",
    "  $$\n",
    "  \\text{Remainder}(A) = \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} \\cdot H(D_v)\n",
    "  $$\n",
    "\n",
    "  onde $ D_v $ representa o subconjunto de dados para os quais o atributo $ A = v $.\n",
    "\n",
    "- Finalmente, o ganho é:\n",
    "\n",
    "  $$\n",
    "  \\text{Gain}(A) = H(D) - \\text{Remainder}(A)\n",
    "  $$\n",
    "\n",
    "Como não há limiar em atributos discretos, o segundo valor devolvido pela função é `None`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0050cc3",
   "metadata": {},
   "source": [
    "## Geração de Regras a Partir da Árvore - `build_rules`\n",
    "\n",
    "Uma das grandes vantagens da utilização de Decision Trees  é a sua **capacidade de explicação**. A função `build_rules` tem como objetivo transformar a árvore gerada pelo algoritmo ID3 numa **lista de regras legíveis**, onde cada regra corresponde a um caminho da raiz até uma folha, com as respetivas condições e classificação final.\n",
    "\n",
    "```python\n",
    "def build_rules(self, tree=None, premises=None):\n",
    "    \"\"\"\n",
    "    Build a list of rules from the decision tree.\n",
    "    - tree: The decision tree (default is the trained tree).\n",
    "    - premises: List of premises leading to the current node.\n",
    "    \"\"\"\n",
    "    tree = self.tree if tree is None else tree\n",
    "    premises = premises or []\n",
    "    rules = []\n",
    "\n",
    "    for node, branches in tree.items():\n",
    "        for value, subtree in branches.items():\n",
    "            # Add the current condition to the premises\n",
    "            new_premise = premises + [(node.attribute, value, node.threshold) if node.threshold is not None else (node.attribute, '=', value)]\n",
    "            if isinstance(subtree, dict):\n",
    "                # Recursively build rules for subtrees\n",
    "                rules.extend(self.build_rules(subtree, new_premise))\n",
    "            else:\n",
    "                # Create a rule for a leaf node\n",
    "                rules.append(Rule(self.attributes, new_premise, subtree))\n",
    "    return rules\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Explicação Técnica\n",
    "\n",
    "#### Parâmetros:\n",
    "\n",
    "- **`tree`**: Subárvore atual. Caso não seja fornecida, utiliza-se a árvore completa treinada (`self.tree`).\n",
    "- **`premises`**: Lista de condições acumuladas ao longo do caminho da raiz até ao nó atual. Cada condição é armazenada como uma tupla.\n",
    "\n",
    "#### Objetivo:\n",
    "\n",
    "Gerar uma lista de objetos da classe `Rule`, onde cada objeto representa uma regra da forma:\n",
    "\n",
    "> **SE** (atributo1 = valor1) **E** (atributo2 ≥ threshold2) **ENTÃO** classe = X\n",
    "\n",
    "\n",
    "\n",
    "### Processo Recursivo\n",
    "\n",
    "1. **Iterar sobre os nós da árvore**:\n",
    "   A árvore é representada como um dicionário onde cada chave é um `Node` e os valores são os ramos descendentes desse nó.\n",
    "\n",
    "2. **Construção de condições (premises)**:\n",
    "   Cada nó adiciona uma nova condição à lista de `premises`. A condição é construída de forma diferente consoante se trata de um atributo contínuo (com `threshold`) ou discreto.\n",
    "\n",
    "   - Para contínuos:  \n",
    "     ```python\n",
    "     (atributo, operador, threshold)\n",
    "     ```\n",
    "     onde `operador` será `'<'` ou `'>='` consoante o ramo.\n",
    "\n",
    "   - Para discretos:\n",
    "     ```python\n",
    "     (atributo, '=', valor)\n",
    "     ```\n",
    "\n",
    "3. **Verificação do tipo de ramo**:\n",
    "   - Se o ramo ainda for um dicionário (`dict`), significa que há mais subdivisões, e a função é chamada recursivamente.\n",
    "   - Se for um valor (rótulo), significa que foi alcançada uma **folha**, e uma nova regra é criada com o conjunto atual de premissas.\n",
    "\n",
    "\n",
    "\n",
    "### Exemplo de Regra Gerada\n",
    "\n",
    "Suponhamos que a árvore contenha os seguintes ramos:\n",
    "\n",
    "- `Node(attribute='coluna_1', threshold=0.5)`:\n",
    "  - Ramo `>=`: vai para `Node(attribute='coluna_3', threshold=None)`\n",
    "    - Ramo `'=': 1` → classe `Jogador_1`\n",
    "    - Ramo `'=': 0` → classe `Jogador_2`\n",
    "\n",
    "Neste caso, as regras geradas seriam algo do género:\n",
    "\n",
    "- SE `coluna_1 ≥ 0.5` E `coluna_3 = 1` → `Classe = Jogador_1`\n",
    "- SE `coluna_1 ≥ 0.5` E `coluna_3 = 0` → `Classe = Jogador_2`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a67cc",
   "metadata": {},
   "source": [
    "## Conclusão - Algoritmo ID3\n",
    "\n",
    "A aplicação do algoritmo **ID3** no contexto do jogo *4 em linha* permitiu uma primeira aproximação à criação de um modelo supervisionado com base em regras explícitas. Apesar da sua simplicidade, o ID3 revelou-se pouco eficaz quando aplicado diretamente sobre o **dataset derivado de estados de jogo simulados por MCTS**.\n",
    "\n",
    "\n",
    "\n",
    "### Resultados e Observações\n",
    "\n",
    "Durante os testes realizados, o modelo alcançou uma **accuracy entre 30% a 40% em dados de teste**, enquanto apresentava **valores superiores a 90% nos dados de treino**. Este comportamento revela a ocorrência de dois problemas fundamentais:\n",
    "\n",
    "- **Overfitting (Alta Variância)**: O modelo ajusta-se excessivamente aos dados de treino, perdendo a capacidade de generalizar para novas situações.\n",
    "- **Alta Bias estrutural**: A simplicidade do ID3 impossibilita uma estratégia mais complexa necessária para uma boa árvore de decisão no jogo de 4 em linha, especialmente em cenários com múltiplas interdependências entre jogadas.\n",
    "\n",
    "\n",
    "\n",
    "### Considerações sobre a Natureza do Problema\n",
    "\n",
    "Estes resultados eram **esperados**, dado que:\n",
    "\n",
    "- O jogo *4 em linha* é altamente estratégico e **não-linear**, com muitas combinações possíveis de jogadas dependentes do contexto.\n",
    "- O ID3 **não tem memória nem profundidade estratégica**, baseando-se apenas em partições de dados locais, sem considerar consequências futuras.\n",
    "\n",
    "\n",
    "\n",
    "### Caminhos Futuros\n",
    "\n",
    "De forma a **ultrapassar as limitações** observadas com o ID3, foram exploradas outras abordagens mais robustas e adequadas à natureza do problema:\n",
    "\n",
    "- **Bagging (Bootstrap Aggregation)**\n",
    "- **RuleSet Generalization** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c00c6e",
   "metadata": {},
   "source": [
    "#  Ruleset (ID3 Tree com Pruning)\n",
    "\n",
    "## Introdução Teórica\n",
    "\n",
    "Um **Ruleset** é uma representação de um modelo de decisão na forma de um conjunto explícito de **regras if-then**, derivadas geralmente de modelos base como **Decision Trees**.\n",
    "\n",
    "No contexto de Supervised Learning, as Decision Trees  como o ID3 podem ser **transformadas num conjunto de regras**, onde cada caminho da raiz até uma folha representa uma regra lógica que conduz a uma classe. \n",
    "\n",
    "### Vantagens desta Implementação\n",
    "- Melhor capacidade de **generalização** por redução do overfitting ao excluir ramos com muito baixo ganho\n",
    "- **Redução** da largura e profundidade da Decision Tree gerada\n",
    "- **Melhoria** da perfomance de classificação\n",
    "\n",
    "\n",
    "### Problemas do Rulesets Implementado\n",
    "- **Custo computacional elevado** - necessário iterar por todas as regras geradas e aferir se o pruning dessa regra afeta, ou não, o ganho de informação da árvore;\n",
    "- **Complexidade Temporal elevada** - quanto maior a árvore gerada, mais tempo demorará a sua poda;\n",
    "\n",
    "\n",
    "## Pruning (Poda)\n",
    "\n",
    "A técnica de **poda** consiste em **remover condições que não gerem ganhos de informação** das regras extraídas, com o objetivo de **simplificar o Ruleset** não compromentedo - e geralmente melhorando - a sua performance. Para isso, o conjunto de dados de treino é divido em train/validation, com ratio 0.67/0.33\n",
    "\n",
    "A poda atua como um mecanismo de **regularização**, reduzindo a complexidade do modelo e prevenindo o overfitting. Os cortes e a  simplificação da árvore de decisão é feita através da seguinte lógica:\n",
    "\n",
    "```python\n",
    "\n",
    "for rule in self.rules:\n",
    "    for _ in range(len(rule.premises)):\n",
    "        acc_before = rule.accuracy(self.prune_data)\n",
    "        removed = rule.premises.pop()  # Try removing the last premise\n",
    "        if acc_before > rule.get_accuracy(self.prune_data):\n",
    "            rule.premises.append(removed)  # Restore if accuracy drops\n",
    "            break\n",
    "\n",
    "```\n",
    "\n",
    "### Explicação da Implementação:\n",
    "\n",
    "- Precorre-se as todas as regras (ramos) da árvore de decisão que queremos podar;\n",
    "- Precorre-se todas as premissas (nós) de uma dada regra;\n",
    "- Calcula-se a accuracy das previsões e, posteriormente, remove-se essa premissa e calcula-se a accuracy pós-remoção;\n",
    "- Se a accuracy antes da remoção for maior do que depois da remoção, volta-se a adcionar a premissa (i.e., a premissa tem elevado ganho de informação para a classificação em validação);\n",
    "- Se a accuracy antes da remoção não for maior, salta-se para a próxima regra (i.e., as premissas subsequentes não garantiam ganhos de informação);\n",
    "\n",
    "\n",
    "\n",
    "Esta abordagem procura um equilíbrio entre **simplicidade** e **eficácia**, e será detalhada nos tópicos seguintes com o respetivo código de implementação e análise dos resultados obtidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d2585",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregation)\n",
    "\n",
    "## Introdução Teórica\n",
    "\n",
    "O **Bagging** (*Bootstrap Aggregation*) é uma técnica de aprendizagem em conjunto (**ensemble learning**) cujo principal objetivo é **reduzir a variância** de modelos de machine learning instáveis, como as Decision Trees .\n",
    "\n",
    "A ideia central do Bagging consiste em:\n",
    "\n",
    "1. **Gerar múltiplos subconjuntos de dados** a partir do conjunto de treino original, usando amostragem com reposição (bootstrap).\n",
    "2. **Treinar modelos independentes** sobre cada um desses subconjuntos.\n",
    "3. **Combinar os resultados** das classificaçõeas dos modelos.\n",
    "\n",
    "Esta abordagem promove a mitigação o problema de **overfitting** típico em modelos altamente sensíveis aos dados, como é o caso do ID3, ao agregar vários modelos que, embora individualmente imperfeitos, se complementam mutuamente.\n",
    "\n",
    "\n",
    "\n",
    "### Justificação da Aplicação no Projeto\n",
    "\n",
    "Como demonstrado no capítulo anterior, a aplicação direta do algoritmo **ID3** ao problema do jogo *4 em linha* resultou numa performance limitada, com clara evidência de **alta variância** - acurácia de treino elevada, mas fraca generalização nos dados de teste.\n",
    "\n",
    "Assim, a técnica de Bagging surge como uma **tentativa natural de aumentar a robustez do modelo** sem alterar o classificador base. Através da combinação de várias árvores ID3 treinadas em subconjuntos diferentes do dataset, esperamos atingir uma **melhoria significativa da performance**, reduzindo a variância sem comprometer em demasia o viés.\n",
    "\n",
    "Nos próximos tópicos será apresentada a implementação desta técnica, bem como os resultados obtidos e comparações com o modelo ID3 isolado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a359700",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "Seguem, abaixo, os resultados retirados dos processos de treino e teste para cada implementação:\n",
    "\n",
    "\n",
    "### **Estatísticas de treino:**\n",
    "Train metrics for ID3 Tree model:<br>\n",
    "***Accuracy**: 1.0000, **Precision**: 1.0000, **Recall**: 1.0000, **F1 Score:** 1.0000*<br><br>\n",
    "Train metrics for Ruleset model:<br>\n",
    "***Accuracy**: 0.7031, **Precision**: 0.7041, **Recall**: 0.7031, **F1 Score**: 0.7010*<br><br>\n",
    "Train metrics for Bagging model:<br>\n",
    "***Accuracy**: 0.6563, **Precision**: 0.6561, **Recall**: 0.6563, **F1 Score**: 0.6540*<br><br>\n",
    "\n",
    "### **Estatísticas de teste:**\n",
    "Test metrics for ID3 Tree model:<br>\n",
    "***Accuracy**: 0.4610, **Precision**: 0.4603, **Recall**: 0.4610, **F1 Score:** 0.4605*<br><br>\n",
    "![](img/cmID3.png)<br><br><br>\n",
    "Test metrics for Ruleset model:<br>\n",
    "***Accuracy**: 0.4279, **Precision**: 0.4207, **Recall**: 0.4279, **F1 Score:** 0.4223*<br><br>\n",
    "![](img/cmRuleset.png)<br><br><br>\n",
    "Test metrics for Bagging model:<br>\n",
    "***Accuracy**: 0.5238, **Precision**: 0.5488, **Recall**: 0.5238, **F1 Score:** 0.5020*<br><br>\n",
    "![](img/cmBagging.png)<br><br><br>\n",
    "\n",
    "\n",
    "*(output retirado do notebook analize_decision_tree.ipynb)*\n",
    "\n",
    "### **Feature importance**\n",
    "\n",
    "Na tentativa de compreender melhor o comportamento dos modelos de Decision Tree implementados e, consequentemente, compreender melhor as decisões do MCTS, estudou-se a `feature importance` dos modelos de Decision Tree. <br> Obtiveram-se os seguintes resultados:\n",
    "\n",
    "##### Feature Importance for ID3 model:<br>\n",
    "\n",
    "| Feature   | Score             |\n",
    "|---------|---------------------|\n",
    "| pieces | 0.07468231995563289 |\n",
    "| cel40  | 0.023394822127791655 |\n",
    "| cel37  | 0.02280822085817135 |\n",
    "| cel42  | 0.022540234662638648 |\n",
    "| cel36  | 0.021436539907826426 |\n",
    "\n",
    "\n",
    "Cels Rank Importance:<br>\n",
    "\n",
    "| 36 | 38 | 37 | 40 | 42 | 41 | 39 |\n",
    "|----|----|----|----|----|----|----|\n",
    "| 30 | 35 | 31 | 32 | 33 | 34 | 29 |\n",
    "| 24 | 28 | 26 | 20 | 27 | 22 | 25 |\n",
    "| 21 | 15 | 14 | 19 | 18 | 16 | 23 |\n",
    "| 12 |  6 |  9 | 17 | 11 |  8 | 10 |\n",
    "|  4 |  2 |  7 | 13 |  1 |  5 |  3 |\n",
    "\n",
    "Column Importances:\n",
    "- **Column 2:** 0.0723 <br>\n",
    "- **Column 6:** 0.0710 <br>\n",
    "- **Column 3:** 0.0694 <br>\n",
    "- **Column 5:** 0.0673 <br>\n",
    "- **Column 7:** 0.0631 <br>\n",
    "- **Column 1:** 0.0616 <br>\n",
    "- **Column 4:** 0.0581 <br>\n",
    "\n",
    "Row Importances:\n",
    "- **Row 6:** 0.1484<br>\n",
    "- **Row 5:** 0.1252<br>\n",
    "- **Row 4:** 0.0897<br>\n",
    "- **Row 3:** 0.0556<br>\n",
    "- **Row 2:** 0.0286<br>\n",
    "- **Row 1:** 0.0153<br>\n",
    "\n",
    "\n",
    "##### Feature Importance for Ruleset model:\n",
    "\n",
    "| Feature | Score |\n",
    "|---------|---------------------|\n",
    "| pieces | 0.03606002481910855 |\n",
    "| cel28  | 0.03518209990704767 |\n",
    "| cel18  | 0.03335682156474307 |\n",
    "| cel19  | 0.029713042215523677 |\n",
    "| cel39  | 0.026297988203348263 |\n",
    "\n",
    "\n",
    "Cels Rank Importance:\n",
    "\n",
    "| 40 | 37 | 39 | 41 | 42 | 36 | 38 |\n",
    "|----|----|----|----|----|----|----|\n",
    "| 32 | 34 | 28 |  6 | 29 | 35 | 33 |\n",
    "| 31 | 30 | 24 |  2 |  3 | 25 | 27 |\n",
    "|  8 | 26 | 19 |  7 | 20 | 22 |  1 |\n",
    "| 21 | 18 | 14 |  5 | 13 | 16 | 23 |\n",
    "| 15 | 12 | 11 |  4 |  9 | 10 | 17 |\n",
    "\n",
    "\n",
    "Column Importances:\n",
    "\n",
    "- **Column 4:** 0.1367  \n",
    "- **Column 5:** 0.0814  \n",
    "- **Column 7:** 0.0651  \n",
    "- **Column 3:** 0.0547  \n",
    "- **Column 1:** 0.0521  \n",
    "- **Column 6:** 0.0500  \n",
    "- **Column 2:** 0.0420  \n",
    "\n",
    "\n",
    "Row Importances:\n",
    "\n",
    "- **Row 6:** 0.1243  \n",
    "- **Row 4:** 0.1205  \n",
    "- **Row 5:** 0.0963  \n",
    "- **Row 3:** 0.0923  \n",
    "- **Row 2:** 0.0438  \n",
    "- **Row 1:** 0.0047  \n",
    "\n",
    "##### Feature Importance for Bagging model:\n",
    "\n",
    "| Feature | Score |\n",
    "|---------|---------------------|\n",
    "| pieces | 0.04132724216288266 |\n",
    "| cel18  | 0.033202479337512324 |\n",
    "| cel28  | 0.032326267456321195 |\n",
    "| cel19  | 0.029609108029982092 |\n",
    "| cel22  | 0.025797087951255744 |\n",
    "\n",
    "\n",
    "Cels Rank Importance:\n",
    "\n",
    "| 42 | 37 | 35 | 39 | 41 | 38 | 40 |\n",
    "|----|----|----|----|----|----|----|\n",
    "| 33 | 36 | 30 |  8 | 24 | 34 | 32 |\n",
    "| 29 | 31 | 25 |  1 |  3 | 28 | 26 |\n",
    "|  4 | 27 | 22 |  5 | 15 | 20 |  2 |\n",
    "| 21 | 19 | 14 |  7 | 13 | 16 | 23 |\n",
    "| 17 | 12 | 10 |  6 |  9 | 11 | 18 |\n",
    "\n",
    "\n",
    "Column Importances:\n",
    "\n",
    "- **Column 4:** 0.1299  \n",
    "- **Column 5:** 0.0873  \n",
    "- **Column 7:** 0.0630  \n",
    "- **Column 3:** 0.0570  \n",
    "- **Column 1:** 0.0541  \n",
    "- **Column 6:** 0.0481  \n",
    "- **Column 2:** 0.0400  \n",
    "\n",
    "\n",
    "Row Importances:\n",
    "\n",
    "- **Row 4:** 0.1220  \n",
    "- **Row 6:** 0.1211  \n",
    "- **Row 5:** 0.0969  \n",
    "- **Row 3:** 0.0911  \n",
    "- **Row 2:** 0.0421  \n",
    "- **Row 1:** 0.0060  \n",
    "<br><br>\n",
    "*(outputs retirados do notebook DT_feature_importance.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb931d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataSci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
